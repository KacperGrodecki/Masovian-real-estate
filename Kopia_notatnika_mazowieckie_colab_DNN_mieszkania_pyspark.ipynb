{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kopia notatnika mazowieckie_colab_DNN_mieszkania_pyspark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNhHXK9/v8uMyLzHBk4gwW1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KacperGrodecki/nieruchomosci-mazowieckie/blob/0.0.4/Kopia_notatnika_mazowieckie_colab_DNN_mieszkania_pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoyxUX4T7Yoj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f554def-bd9f-4fdd-f158-1a9bd6787827"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import svm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "np.random.seed(7)\n",
        "from google.colab import drive\n",
        "import seaborn as sns\n",
        "drive.mount('/content/drive')\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "#from otoDomScraper import daneDomu\n",
        "#from random import randrange\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "import statistics\n",
        "from sklearn.cluster import AffinityPropagation\n",
        "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
        "from sklearn.decomposition import FastICA\n",
        "from sklearn import preprocessing\n",
        "from IPython.display import Javascript\n",
        "import requests\n",
        "from collections import OrderedDict\n",
        "import seaborn as sns\n",
        "import os\n",
        "import ipywidgets as widgets"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltyQbEQQl4Rj"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVcTv4v-mBNw"
      },
      "source": [
        "!wget -q https://dlcdn.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz"
      ],
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEu62hMgmfqn"
      },
      "source": [
        "!tar xf spark-3.1.2-bin-hadoop3.2.tgz"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_i26VG-moxJ"
      },
      "source": [
        "!pip install -q findspark"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_jbyzGfmsUx"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\""
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyiPvfDvmz5s"
      },
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ieA-xGVnm2vq",
        "outputId": "5402ca5f-e139-4cd0-b4cf-b9d531df96e3"
      },
      "source": [
        "findspark.find()"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/spark-3.1.2-bin-hadoop3.2'"
            ]
          },
          "metadata": {},
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bmKsvfkm7MC"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "MqY9cWLZm_f6",
        "outputId": "45719e75-46d4-4955-b78d-744309b0ba1e"
      },
      "source": [
        "spark"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://ab28497b174f:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Colab</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f76513be090>"
            ]
          },
          "metadata": {},
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-WNPMRQoD9E",
        "outputId": "19fad5b5-bcef-4810-bc93-1b1645bd2d35"
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "get_ipython().system_raw('./ngrok http 4050 &')"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-08 12:20:31--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 54.161.241.46, 52.202.168.65, 54.237.133.81, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|54.161.241.46|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13832437 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.2’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.19M  6.63MB/s    in 2.0s    \n",
            "\n",
            "2021-10-08 12:20:34 (6.63 MB/s) - ‘ngrok-stable-linux-amd64.zip.2’ saved [13832437/13832437]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2yEYotJoKhT",
        "outputId": "266b6c5f-09d5-43ff-944f-9d1be4433cb1"
      },
      "source": [
        "!curl -s http://localhost:4040/api/tunnels"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"tunnels\":[{\"name\":\"command_line\",\"uri\":\"/api/tunnels/command_line\",\"public_url\":\"https://223a-34-134-215-57.ngrok.io\",\"proto\":\"https\",\"config\":{\"addr\":\"http://localhost:4050\",\"inspect\":true},\"metrics\":{\"conns\":{\"count\":0,\"gauge\":0,\"rate1\":0,\"rate5\":0,\"rate15\":0,\"p50\":0,\"p90\":0,\"p95\":0,\"p99\":0},\"http\":{\"count\":0,\"rate1\":0,\"rate5\":0,\"rate15\":0,\"p50\":0,\"p90\":0,\"p95\":0,\"p99\":0}}},{\"name\":\"command_line (http)\",\"uri\":\"/api/tunnels/command_line%20%28http%29\",\"public_url\":\"http://223a-34-134-215-57.ngrok.io\",\"proto\":\"http\",\"config\":{\"addr\":\"http://localhost:4050\",\"inspect\":true},\"metrics\":{\"conns\":{\"count\":0,\"gauge\":0,\"rate1\":0,\"rate5\":0,\"rate15\":0,\"p50\":0,\"p90\":0,\"p95\":0,\"p99\":0},\"http\":{\"count\":0,\"rate1\":0,\"rate5\":0,\"rate15\":0,\"p50\":0,\"p90\":0,\"p95\":0,\"p99\":0}}}],\"uri\":\"/api/tunnels\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wy925Vons1Y8"
      },
      "source": [
        "from pyspark.sql.functions import udf"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0g5Z3J9q9cr"
      },
      "source": [
        "def locCities(cityData):\n",
        "  locationCities=[]\n",
        "  for data in cityData:\n",
        "    locationCities.append(data)\n",
        "  locationCities=pd.DataFrame(np.array(locationCities),columns=['cityX','cityY','cityGeo'])\n",
        "  return locationCities\n",
        "\n",
        "def locDistricts(districtsData):\n",
        "  locationCities=[]\n",
        "  for data in cityData:\n",
        "    locationDistricts.append(data)\n",
        "  locationDistricts=pd.DataFrame(np.array(locationDistricts),columns=['cityX','cityY','cityGeo'])\n",
        "  return locationDistricts\n",
        "\n",
        "def locCounties(countyData):\n",
        "  locationCounty=[]\n",
        "  for data in countyData:\n",
        "    locationCounty.append(data)\n",
        "  locationCounty=pd.DataFrame(np.array(locationCounty),columns=['countyX','countyY','countyGeo'])\n",
        "  return locationCounty"
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NB0JTR7rAkB"
      },
      "source": [
        "def selectLocationX(x):\n",
        " # print(x[17],x[20],x[23])\n",
        "  if x[2]<min(x[5],x[8]):\n",
        "    #print('min 17',x[15],x[16])\n",
        "    return x[0]\n",
        "  elif x[5]<min(x[2],x[8]):\n",
        "    #print('min 20 ',x[18],x[19])\n",
        "    return x[3]\n",
        "  elif x[8]<min(x[2],x[5]):\n",
        "    #print('min 23 ',x[21],x[22])\n",
        "    return x[6]\n",
        "\n",
        "def selectLocationY(x):\n",
        " # print(x[17],x[20],x[23])\n",
        "  if x[2]<min(x[5],x[8]):\n",
        "    #print('min 17',x[15],x[16])\n",
        "    return x[1]\n",
        "  elif x[5]<min(x[2],x[8]):\n",
        "    #print('min 20 ',x[18],x[19])\n",
        "    return x[4]\n",
        "  elif x[8]<min(x[2],x[5]):\n",
        "    #print('min 23 ',x[21],x[22])\n",
        "    return x[7]"
      ],
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SN8eAxzVo5Ai"
      },
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType,StringType,FloatType"
      ],
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSoVW51euuF1"
      },
      "source": [
        "def toNum2(txt):\n",
        "    if type(txt) is int:\n",
        "        return txt\n",
        "    elif (type(txt) is str):\n",
        "        digs = re.findall(r'\\d+', txt)\n",
        "        if len(digs) == 1:\n",
        "            return int(digs[0])\n",
        "        elif len(digs) == 2:\n",
        "            return 1000 * int(digs[0]) + int(digs[1])\n",
        "        elif len(digs) == 3:\n",
        "            return 1000000 * int(digs[0]) + 1000 * int(digs[1]) + int(digs[0])\n",
        "\n",
        "    #   return int(digs)\n",
        "\n",
        "udf_toNum2 = udf(toNum2, IntegerType())\n",
        "\n",
        "\n",
        "def toNum1(txt):\n",
        "    print(txt)\n",
        "    if type(txt) is str:\n",
        "        digs = re.findall(r'\\d+', txt)\n",
        "        if len(digs) == 1:\n",
        "            return float(digs[0])\n",
        "        elif len(digs) == 2 and (txt[1] != ' '):\n",
        "            return float(digs[0]) + 0.01 *float(digs[1])\n",
        "        elif len(digs) == 3:\n",
        "            return 1000 * float(digs[0]) + float(digs[1]) + 0.001 * float(digs[2])\n",
        "        elif (type(txt) is str) and (txt[1] == ' '):\n",
        "            digs = re.findall(r'\\d+', txt)\n",
        "            return 1000 * float(digs[0]) + float(digs[1])\n",
        "    else:\n",
        "        return txt\n",
        "\n",
        "udf_toNum1 = udf(toNum1, FloatType())\n",
        "\n",
        "\n",
        "def toNum3(txt):\n",
        "    if type(txt) == int:\n",
        "        return txt\n",
        "    elif type(txt) == float:\n",
        "      if np.isnan(txt):\n",
        "        return 0\n",
        "      else:\n",
        "        return int(txt)\n",
        "    elif type(txt) == str:\n",
        "      return float(re.findall(r'\\d+', txt)[0])\n",
        "    else:\n",
        "      return float(0)\n",
        "\n",
        "udf_toNum3 = udf(toNum3, FloatType())\n",
        "\n",
        "def pietra(txt):\n",
        "    if type(txt) is str:\n",
        "        if '0' in txt:\n",
        "            return 0\n",
        "        if '1' in txt:\n",
        "            return 1\n",
        "        elif '2' in txt:\n",
        "            return 2\n",
        "        elif '3' in txt:\n",
        "            return 3\n",
        "        elif 'parterowy' in txt:\n",
        "            return 0\n",
        "    elif type(txt)==float:\n",
        "      if np.isnan(txt):\n",
        "        return 0\n",
        "      else:\n",
        "        return int(txt)\n",
        "    elif type(txt)==int:\n",
        "      return txt\n",
        "    else:\n",
        "      return txt\n",
        "\n",
        "udf_pietra = udf(pietra, IntegerType())      \n",
        "\n",
        "def cities(x):\n",
        "    dist=x.split()[4]\n",
        "    #city=x.split()[5]\n",
        "    if dist=='warszawski':\n",
        "        return x.split()[6].lower()\n",
        "    elif dist in ['Warszawa','Radom','Płock','Siedlce']:\n",
        "        return dist\n",
        "    else:\n",
        "        try:\n",
        "            return x.split()[5].lower()\n",
        "        except:\n",
        "            return 'unknown'\n",
        "\n",
        "udf_cities = udf(cities, StringType())      \n",
        "\n",
        "\n",
        "def region(x):\n",
        "    if x.split()[4]=='Warszawa':\n",
        "        try:\n",
        "            return x.split()[5].lower()\n",
        "        except:\n",
        "            return ''\n",
        "    else:\n",
        "        return ''\n",
        "\n",
        "udf_region = udf(region, StringType())    "
      ],
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdQtbHxQ9a8I"
      },
      "source": [
        "from pyspark.sql.functions import split\n",
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.sql.functions import when"
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgzx_S2da84w"
      },
      "source": [
        "dfCities=spark.read.csv('/content/drive/MyDrive/domymazowieckie/dfcities.csv')\n",
        "dfCities1=pd.read_csv('/content/drive/MyDrive/domymazowieckie/dfcities.csv')\n",
        "\n",
        "dfCounties=spark.read.csv('/content/drive/MyDrive/domymazowieckie/dfCounties.csv')\n",
        "dfCounties1=pd.read_csv('/content/drive/MyDrive/domymazowieckie/dfCounties.csv')\n",
        "\n",
        "dfVoivodeships=spark.read.csv('/content/drive/MyDrive/domymazowieckie/dfVoivodeships.csv')\n",
        "\n",
        "dfWarsawDistricts=spark.read.csv('/content/drive/MyDrive/domymazowieckie/dfWarsawDistricts.csv')\n",
        "dfWarsawDistricts1=pd.read_csv('/content/drive/MyDrive/domymazowieckie/dfWarsawDistricts.csv')"
      ],
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVGe89zBKbUJ"
      },
      "source": [
        "def funCities(x):\n",
        "  print('function')\n",
        "  city=x\n",
        "  rows=dfCities1.loc[dfCities1['city']==city]\n",
        "  precision=0\n",
        "  print(x)\n",
        "  if rows.shape[0] == 0:\n",
        "    print('row 0')\n",
        "    return float(0) #0,0,100\n",
        "  elif rows.shape[0] == 1:\n",
        "    i=0\n",
        "    print('row 1')\n",
        "    longitude=rows.iloc[0,2]#rows.collect()[0][2]\n",
        "    latitude=rows.iloc[0,1]#rows.collect()[0][1]\n",
        "    precision=rows.iloc[0,3]#rows.collect()[0][3]\n",
        "    return float(longitude) #longitude[0],latitude[0],precision[0]\n",
        "  elif rows.shape[0] > 1:\n",
        "    print('row >1')\n",
        "    prec1,prec2=rows.iloc[0,3] ,rows.iloc[1,3] #rows.collect()[0][3],rows.collect()[1][3]\n",
        "    print('funCities',prec1,prec2)\n",
        "    if prec1<prec2:\n",
        "      precision=prec1\n",
        "      longitude=rows.iloc[0,2]\n",
        "      latitude=rows.iloc[0,1]\n",
        "    else:\n",
        "      precision=prec2\n",
        "      longitude=rows.iloc[1,2]\n",
        "      latitude=rows.iloc[1,1]\n",
        "    return float(longitude) #longitude,latitude,precision\n",
        "  else:\n",
        "    return float(0) \n",
        "\n",
        "udf_funCities = udf(funCities, FloatType())  \n",
        "\n",
        "def funCities1(x):\n",
        "  print('function')\n",
        "  rows=dfCities[dfCities['_c0']==x.lower()]\n",
        "  precision=0\n",
        "  print(x)\n",
        "  if rows.count() == 0:\n",
        "    print('row 0')\n",
        "    return 0 #0,0,100\n",
        "  elif rows.count() == 1:\n",
        "    print('row 1')\n",
        "    longitude=rows.collect()[0][2]\n",
        "    latitude=rows.collect()[0][1]\n",
        "    precision=rows.collect()[0][3]\n",
        "    return longitude[0] #longitude[0],latitude[0],precision[0]\n",
        "  elif rows.count() > 1:\n",
        "    print('row >1')\n",
        "    prec1,prec2=rows.collect()[0][3],rows.collect()[1][3]#rows.iloc[0,3],rows.iloc[1,3]\n",
        "    if prec1<prec2:\n",
        "      precision=prec1\n",
        "      longitude=rows.collect()[0][2]\n",
        "      latitude=rows.collect()[0][1]\n",
        "    else:\n",
        "      precision=prec2\n",
        "      longitude=rows.collect()[1][2]\n",
        "      latitude=rows.collect()[1][1]\n",
        "    return longitude #longitude,latitude,precision\n",
        "  else:\n",
        "    return 0 \n",
        "  \n",
        "def funDistricts(x):\n",
        "  rows=dfWarsawDistricts1[dfWarsawDistricts1['warsaw_district']==x]\n",
        "  #print(x, ' ',rows.shape[0])\n",
        "  if rows.shape[0]==0:\n",
        "    return 0,0,100\n",
        "  elif rows.shape[0]==1:\n",
        "    longitude=rows.longitude.values\n",
        "    latitude=rows.latitude.values\n",
        "    precision=rows.geoPrecision.values\n",
        "    return longitude[0],latitude[0],precision[0]\n",
        "\n",
        "udf_funDistricts = udf(funDistricts, FloatType()) \n",
        "\n",
        "\n",
        "def funCounties(x):\n",
        "  try:\n",
        "    rows=dfCounties1['powiat '+x== dfCounties1['county']]\n",
        "  except Exception as e:\n",
        "    return e\n",
        "  #print(x, ' ',rows.shape[0])\n",
        "  if rows.shape[0]==0:\n",
        "    return 0,0,100\n",
        "  elif rows.shape[0]==1:\n",
        "    longitude=rows.longitude.values\n",
        "    latitude=rows.latitude.values\n",
        "    precision=rows.geoPrecision.values\n",
        "    return longitude[0],latitude[0],precision[0]\n",
        "udf_funCounties = udf(funCounties, FloatType())   "
      ],
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "W6EhzkgmOKqE",
        "outputId": "fe782947-4d15-44a6-98b3-12a30140b8c3"
      },
      "source": [
        "dfCities1.head()"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>city</th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "      <th>geoPrecision</th>\n",
              "      <th>wikidata_item_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>aleksandrów łódzki</td>\n",
              "      <td>51.816667</td>\n",
              "      <td>19.300000</td>\n",
              "      <td>0.016667</td>\n",
              "      <td>Q664371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>alwernia</td>\n",
              "      <td>50.060560</td>\n",
              "      <td>19.539530</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>Q983896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>alwernia</td>\n",
              "      <td>50.066667</td>\n",
              "      <td>19.516667</td>\n",
              "      <td>0.016667</td>\n",
              "      <td>Q983896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>andrychów</td>\n",
              "      <td>49.854970</td>\n",
              "      <td>19.338340</td>\n",
              "      <td>0.139333</td>\n",
              "      <td>Q984857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>annopol</td>\n",
              "      <td>50.883333</td>\n",
              "      <td>21.866667</td>\n",
              "      <td>0.016667</td>\n",
              "      <td>Q567332</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0                city  ...  geoPrecision  wikidata_item_id\n",
              "0           0  aleksandrów łódzki  ...      0.016667           Q664371\n",
              "1           1            alwernia  ...      0.000010           Q983896\n",
              "2           2            alwernia  ...      0.016667           Q983896\n",
              "3           3           andrychów  ...      0.139333           Q984857\n",
              "4           4             annopol  ...      0.016667           Q567332\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyCylFst7j1X"
      },
      "source": [
        "def makeDataFrame(file):\n",
        "\n",
        "    schema = StructType([\n",
        "    StructField(\"id\", IntegerType(), True),                \n",
        "    StructField(\"dzielnica\", StringType(), True),\n",
        "    StructField(\"powierzchnia\", StringType(), True),\n",
        "    StructField(\"lPokoi\",FloatType(), True),\n",
        "    StructField(\"powierzchniaDzialki\", FloatType(), True),\n",
        "    StructField(\"rodzajZabudowy\", StringType(), True),\n",
        "    StructField(\"materialBudynku\", StringType(), True),\n",
        "    StructField(\"rokBudowy\", FloatType(), True),\n",
        "    StructField(\"stanWykonczenia\", StringType(), True),\n",
        "    StructField(\"okna\", StringType(), True),\n",
        "    StructField(\"rynek\", StringType(), True),\n",
        "    StructField(\"lPieter\", FloatType(), True),\n",
        "    StructField(\"cena\", StringType(), True),])\n",
        "    concat=spark.read.csv(file,schema=schema)\n",
        "    #########\n",
        "    print(type(concat))\n",
        "    #concat['powierzchnia_corr'] = concat['powierzchnia'].apply(lambda x: toNum1(x))\n",
        "    #df = df.withColumn(\"message\", udf_myFunction(\"_3\"))\n",
        "    concat= concat.withColumn('powierzchnia_corr',udf_toNum1('powierzchnia'))\n",
        "    print('powierzchnia_corr')\n",
        "    #concat.show()\n",
        "    #####\n",
        "    #concat['powierzchniaDzialki_corr'] = concat['powierzchniaDzialki'].apply(lambda x: toNum2(x))\n",
        "    concat= concat.withColumn('powierzchniaDzialki_corr',udf_toNum2('powierzchniaDzialki'))\n",
        "    print('powierzchniaDzialki_corr')\n",
        "    #concat.show()\n",
        "    #concat['cena_corr'] = concat['cena'].apply(lambda x: toNum2(x))\n",
        "    concat= concat.withColumn('cena_corr',udf_toNum2('cena'))\n",
        "    print('cena_corr')\n",
        "    #concat.show()\n",
        "    #concat['rokBudowy_corr'] = concat['rokBudowy'].apply(lambda x: toNum3(x))\n",
        "    concat= concat.withColumn('rokBudowy_corr',udf_toNum3('rokBudowy'))\n",
        "    print('rokBudowy_corr')\n",
        "    #concat.show()\n",
        "    #concat['cena/m'] = concat['cena_corr'] / concat['powierzchnia_corr']####??\n",
        "    concat= concat.withColumn('cena/m',concat['cena_corr']/concat['powierzchnia_corr'])\n",
        "    print('cena/m')\n",
        "    #concat.show()\n",
        "    #concat['lPieter_crr'] = concat['lPieter'].apply(lambda x: pietra(x))\n",
        "    concat= concat.withColumn('lPieter_crr',udf_pietra('lPieter'))\n",
        "    print('lPieter_crr')\n",
        "   # concat.show()\n",
        "    concat = concat[concat['powierzchnia_corr'] > 0]\n",
        "    concat = concat[concat['cena_corr'] > 0]\n",
        "    print('filtr powierzchnia i cena')\n",
        "   # concat.show()\n",
        "    #concat['lPokoi'] = concat['lPokoi'].apply(lambda x: toNum3(x))\n",
        "    concat= concat.withColumn('lPokoi',udf_toNum3('lPokoi'))\n",
        "  #  concat.show()\n",
        "    #concat['districts']=concat['dzielnica'].apply(lambda x: x.split()[4])\n",
        "    ##https://stackoverflow.com/questions/39235704/split-spark-dataframe-string-column-into-multiple-columns\n",
        "    dzielnica = split(concat['dzielnica'], ' ')\n",
        "    concat = concat.withColumn('districts', dzielnica.getItem(4))\n",
        "    print('districts')\n",
        "   # concat.show()    \n",
        "    concat= concat.withColumn('cities_corr',udf_cities('dzielnica'))\n",
        "    print('cities_corr')\n",
        "   # concat.show()        \n",
        "    #concat['cities_corr']=cities_corr\n",
        "\n",
        "    #region_corr=concat['dzielnica'].apply(lambda x: region(x))\n",
        "    concat= concat.withColumn('region_corr',udf_region('dzielnica'))\n",
        "   # concat['region_corr']=region_corr\n",
        "    print('region_corr')\n",
        "   # concat.show()  \n",
        "\n",
        "    concat_dropped = concat.drop('dzielnica', 'powierzchnia', 'powierzchniaDzialki', 'lPieter', 'cena', 'cena_corr')\n",
        "    print('concat_dropped')\n",
        "    concat_dropped.show()  \n",
        "    \n",
        "    #data_df = data_df.withColumn(\"Plays\", data_df[\"Plays\"].cast(IntegerType()))\n",
        "    \n",
        "    #concat_dropped['rokBudowy_corr'] = concat_dropped['rokBudowy_corr'].astype('int')\n",
        "    concat_dropped= concat.withColumn('rokBudowy_corr',concat['rokBudowy_corr'].cast(IntegerType()))\n",
        "    print('rokBudowy_corr')\n",
        "    #concat_dropped.show()  \n",
        "    concat_dropped = concat_dropped.withColumn(\"rokBudowy_corr\", when(concat_dropped[\"rokBudowy_corr\"] < 0, 1900).otherwise(concat_dropped[\"rokBudowy_corr\"]))\n",
        "    #concat_dropped[concat_dropped['rokBudowy_corr']<1900,'rokBudowy_corr']=1980\n",
        "    print('korekta na rok')\n",
        "    #concat_dropped.show()  \n",
        "    #concat_dropped[concat_dropped['rokBudowy_corr']>2030,'rokBudowy_corr']=2020\n",
        "    concat_dropped = concat_dropped.withColumn(\"rokBudowy_corr\", when(concat_dropped[\"rokBudowy_corr\"] > 2030, 2020).otherwise(concat_dropped[\"rokBudowy_corr\"]))\n",
        "    #concat_dropped = concat_dropped[concat_dropped['rokBudowy_corr'] < 2030]\n",
        "    concat_dropped = concat_dropped.fillna(0)\n",
        "    print('fill nan')\n",
        "    #concat_dropped.show()  \n",
        "    concat_dropped = concat_dropped[concat_dropped['cena/m'] < 20000]\n",
        "    print('korekta na cene')\n",
        "    #concat_dropped.show()  \n",
        "    \n",
        "    #cityData=concat_dropped.cities_corr.apply(lambda x: funCities(x))\n",
        "    concat_dropped= concat_dropped.withColumn(\"locationCities\",udf_funCities(\"cities_corr\"))\n",
        "    #https://stackoverflow.com/questions/38984775/spark-errorexpected-zero-arguments-for-construction-of-classdict-for-numpy-cor\n",
        "    #locationCities=locCities(cityData)\n",
        "    print('locationCities')\n",
        "    concat_dropped.show()\n",
        "    #districtsData=concat_dropped.region_corr.apply(lambda x: funDistricts(x))\n",
        "    concat_dropped= concat_dropped.withColumn(\"locationDistricts\",udf_funDistricts(\"region_corr\"))\n",
        "    #locationDistricts=locCities(districtsData)\n",
        "    print('locationDistricts')\n",
        "    concat_dropped.show()\n",
        "    #countyData=concat_dropped.districts.apply(lambda x: funCounties(x))\n",
        "    concat_dropped= concat_dropped.withColumn(\"locationCounty\",udf_funCounties(\"districts\"))\n",
        "    print('locationCounty')\n",
        "    concat_dropped.show()\n",
        "    #locationCounty=locCounties(countyData)\n",
        "    print('locations')\n",
        "    concat_dropped.show()\n",
        "    concat_dropped_reset=concat_dropped.drop('id','rokBudowy', 'districts', 'cities_corr', 'region_corr')\n",
        "    concat_dropped_reset.show()\n",
        "    #https://stackoverflow.com/questions/35804755/apply-onehotencoder-for-several-categorical-columns-in-sparkmlib\n",
        "    #https://stackoverflow.com/questions/42805663/e-num-get-dummies-in-pyspark\n",
        "\n",
        "    rodzajZabudowy = concat_dropped_reset_drop.select(\"rodzajZabudowy\").distinct().rdd.flatMap(lambda x: x).collect()\n",
        "    materialBudynku = concat_dropped_reset_drop.select(\"materialBudynku\").distinct().rdd.flatMap(lambda x: x).collect()\n",
        "    stanWykonczenia = concat_dropped_reset_drop.select(\"stanWykonczenia\").distinct().rdd.flatMap(lambda x: x).collect()\n",
        "    okna = concat_dropped_reset_drop.select(\"okna\").distinct().rdd.flatMap(lambda x: x).collect()\n",
        "    rynek = concat_dropped_reset_drop.select(\"rynek\").distinct().rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "\n",
        "    rodzajZabudowy_expr = [F.when(F.col(\"rodzajZabudowy\") == code, 1).otherwise(0).alias(\"e_rodzajZabudowy_\" + code) for code in rodzajZabudowy]\n",
        "    materialBudynku_expr = [F.when(F.col(\"materialBudynku\") == code, 1).otherwise(0).alias(\"e_materialBudynku_\" + code) for code in materialBudynku]\n",
        "    stanWykonczenia_expr = [F.when(F.col(\"stanWykonczenia\") == code, 1).otherwise(0).alias(\"e_stanWykonczenia_\" + code) for code in stanWykonczenia]\n",
        "    okna_expr = [F.when(F.col(\"okna\") == code, 1).otherwise(0).alias(\"e_okna_\" + code) for code in okna]\n",
        "    rynek_expr = [F.when(F.col(\"rynek\") == code, 1).otherwise(0).alias(\"e_rynek_\" + code) for code in rynek]\n",
        "\n",
        "\n",
        "\n",
        "    concat_dummies = concat_dropped_reset_drop.select('rodzajZabudowy',*rodzajZabudowy_expr+materialBudynku_expr+stanWykonczenia_expr+okna_expr+rynek_expr)\n",
        "    concat_dummies.show()\n",
        "    #final = pd.get_dummies(concat_dropped_reset_drop, columns=['rodzajZabudowy', 'materialBudynku', 'stanWykonczenia', 'okna', 'rynek'])\n",
        "\n",
        "    return concat_dummies"
      ],
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmOjGx4Z7lbR"
      },
      "source": [
        "filename='/content/drive/MyDrive/domymazowieckie/dfMieszkania1.csv'"
      ],
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hl-GYCGD7mwf",
        "outputId": "e938fd34-a233-47d9-bb64-64a8381c8e4a"
      },
      "source": [
        "final=makeDataFrame(filename)"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.sql.dataframe.DataFrame'>\n",
            "powierzchnia_corr\n",
            "powierzchniaDzialki_corr\n",
            "cena_corr\n",
            "rokBudowy_corr\n",
            "cena/m\n",
            "lPieter_crr\n",
            "filtr powierzchnia i cena\n",
            "districts\n",
            "cities_corr\n",
            "region_corr\n",
            "concat_dropped\n",
            "+----+------+---------------+---------------+---------+---------------+----------+---------+-----------------+------------------------+--------------+------------------+-----------+-----------+-----------+------------+\n",
            "|  id|lPokoi| rodzajZabudowy|materialBudynku|rokBudowy|stanWykonczenia|      okna|    rynek|powierzchnia_corr|powierzchniaDzialki_corr|rokBudowy_corr|            cena/m|lPieter_crr|  districts|cities_corr| region_corr|\n",
            "+----+------+---------------+---------------+---------+---------------+----------+---------+-----------------+------------------------+--------------+------------------+-----------+-----------+-----------+------------+\n",
            "|null|  null|           blok|           inne|   2012.0|do zamieszkania|plastikowe|   wtórny|             59.7|                    null|          null|16566.163942395644|          6|   Warszawa|   Warszawa|     bielany|\n",
            "|   0|  null|apartamentowiec|           null|      0.0|           null|      null|pierwotny|            44.44|                    null|          null|11300.000349194172|          3|   Warszawa|   Warszawa|      włochy|\n",
            "|   1|  null|apartamentowiec|         pustak|   2004.0|do zamieszkania| drewniane|   wtórny|            146.0|                    null|          null|24657.554794520547|          4|   Warszawa|   Warszawa|     mokotów|\n",
            "|   3|  null|           null|           null|   2021.0|           null|      null|pierwotny|             91.0|                    null|          null|13571.439560439561|          0|   Warszawa|   Warszawa|        wola|\n",
            "|   4|  null|           null|           null|   1977.0|do zamieszkania|      null|   wtórny|             38.0|                    null|          null|12763.157894736842|         12|   Warszawa|   Warszawa|     bielany|\n",
            "|   5|  null|           blok|         pustak|   2022.0| do wykończenia| drewniane|pierwotny|             74.5|                    null|          null|  8912.75167785235|          3|   Warszawa|   Warszawa|       ursus|\n",
            "|   6|  null|apartamentowiec|           null|   2004.0|do zamieszkania|aluminiowe|   wtórny|            107.4|                    null|          null|12942.281007929669|         10|   Warszawa|   Warszawa| śródmieście|\n",
            "|   7|  null|           blok|   wielka płyta|   1970.0|     do remontu|plastikowe|   wtórny|             61.7|                    null|          null| 9708.265682223357|         10|   Warszawa|   Warszawa|     bielany|\n",
            "|   8|  null|           null|           null|   2004.0|do zamieszkania|      null|   wtórny|             82.0|                    null|          null|14085.378048780487|         16|   Warszawa|   Warszawa|      ochota|\n",
            "|   9|  null|      kamienica|          cegła|   1938.0|do zamieszkania|plastikowe|   wtórny|             26.9|                    null|          null| 14498.14146953911|          4|   Warszawa|   Warszawa|praga-północ|\n",
            "|  11|  null|           blok|           null|   1998.0|           null|      null|   wtórny|            80.02|                    null|          null|11247.188674782123|         17|   Warszawa|   Warszawa|        wola|\n",
            "|  12|  null|           blok|          cegła|   2001.0|do zamieszkania|plastikowe|   wtórny|             55.9|                    null|          null|10178.890598716378|          4|   Warszawa|   Warszawa|    targówek|\n",
            "|  13|  null|           blok|           null|   2003.0|do zamieszkania|plastikowe|   wtórny|             47.0|                    null|          null|10617.021276595744|          3|   Warszawa|   Warszawa|     bielany|\n",
            "|  14|  null|apartamentowiec|           null|   2009.0|do zamieszkania|      null|   wtórny|            30.33|                    null|          null|15792.944319317707|         10|   Warszawa|   Warszawa|     mokotów|\n",
            "|  16|  null|      kamienica|          cegła|   1960.0|     do remontu|plastikowe|   wtórny|            34.95|                    null|          null|13991.416003587889|          4|   Warszawa|   Warszawa|     mokotów|\n",
            "|  18|  null|           blok|           null|      0.0|do zamieszkania|plastikowe|pierwotny|             71.5|                    null|          null| 5552.447552447553|          3|legionowski|  wieliszew|            |\n",
            "|  19|  null|apartamentowiec|           null|   2020.0|           null|plastikowe|   wtórny|             74.0|                    null|          null|12635.121621621622|          8|   Warszawa|   Warszawa|     mokotów|\n",
            "|  21|  null|           null|           null|   2022.0| do wykończenia|plastikowe|pierwotny|            35.24|                    null|          null| 10868.33092388954|          8|   Warszawa|   Warszawa|   białołęka|\n",
            "|  22|  null|           blok|           inne|      0.0| do wykończenia|plastikowe|pierwotny|             83.8|                    null|          null| 9299.999661320446|          3|   Warszawa|   Warszawa|       ursus|\n",
            "|  23|  null|apartamentowiec|         żelbet|   2021.0| do wykończenia|aluminiowe|pierwotny|             75.0|                    null|          null|13186.666666666666|         23|   Warszawa|   Warszawa|        wola|\n",
            "+----+------+---------------+---------------+---------+---------------+----------+---------+-----------------+------------------------+--------------+------------------+-----------+-----------+-----------+------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "rokBudowy_corr\n",
            "korekta na rok\n",
            "fill nan\n",
            "korekta na cene\n",
            "locationCities\n",
            "+---+--------------------+------------+------+-------------------+---------------+---------------+---------+---------------+----------+---------+-------+------------+-----------------+------------------------+---------+--------------+------------------+-----------+-----------+-----------+------------+--------------+\n",
            "| id|           dzielnica|powierzchnia|lPokoi|powierzchniaDzialki| rodzajZabudowy|materialBudynku|rokBudowy|stanWykonczenia|      okna|    rynek|lPieter|        cena|powierzchnia_corr|powierzchniaDzialki_corr|cena_corr|rokBudowy_corr|            cena/m|lPieter_crr|  districts|cities_corr| region_corr|locationCities|\n",
            "+---+--------------------+------------+------+-------------------+---------------+---------------+---------+---------------+----------+---------+-------+------------+-----------------+------------------------+---------+--------------+------------------+-----------+-----------+-----------+------------+--------------+\n",
            "|  0|Mieszkanie na spr...|    59,70 m²|   0.0|                0.0|           blok|           inne|   2012.0|do zamieszkania|plastikowe|   wtórny|    6.0|  989 000 zł|             59.7|                       0|   989000|             0|16566.163942395644|          6|   Warszawa|   Warszawa|     bielany|           0.0|\n",
            "|  0|Mieszkanie na spr...|    44,44 m²|   0.0|                0.0|apartamentowiec|           null|      0.0|           null|      null|pierwotny|    3.0|  502 172 zł|            44.44|                       0|   502172|             0|11300.000349194172|          3|   Warszawa|   Warszawa|      włochy|           0.0|\n",
            "|  3|Mieszkanie na spr...|       91 m²|   0.0|                0.0|           null|           null|   2021.0|           null|      null|pierwotny|    0.0|1 235 000 zł|             91.0|                       0|  1235001|             0|13571.439560439561|          0|   Warszawa|   Warszawa|        wola|           0.0|\n",
            "|  4|Mieszkanie na spr...|       38 m²|   0.0|                0.0|           null|           null|   1977.0|do zamieszkania|      null|   wtórny|   12.0|  485 000 zł|             38.0|                       0|   485000|             0|12763.157894736842|         12|   Warszawa|   Warszawa|     bielany|           0.0|\n",
            "|  5|Mieszkanie na spr...|    74,50 m²|   0.0|                0.0|           blok|         pustak|   2022.0| do wykończenia| drewniane|pierwotny|    3.0|  664 000 zł|             74.5|                       0|   664000|             0|  8912.75167785235|          3|   Warszawa|   Warszawa|       ursus|           0.0|\n",
            "|  6|Mieszkanie na spr...|   107,40 m²|   0.0|                0.0|apartamentowiec|           null|   2004.0|do zamieszkania|aluminiowe|   wtórny|   10.0|1 390 830 zł|            107.4|                       0|  1390001|             0|12942.281007929669|         10|   Warszawa|   Warszawa| śródmieście|           0.0|\n",
            "|  7|Mieszkanie na spr...|    61,70 m²|   0.0|                0.0|           blok|   wielka płyta|   1970.0|     do remontu|plastikowe|   wtórny|   10.0|  599 000 zł|             61.7|                       0|   599000|             0| 9708.265682223357|         10|   Warszawa|   Warszawa|     bielany|           0.0|\n",
            "|  8|Mieszkanie na spr...|       82 m²|   0.0|                0.0|           null|           null|   2004.0|do zamieszkania|      null|   wtórny|   16.0|1 155 000 zł|             82.0|                       0|  1155001|             0|14085.378048780487|         16|   Warszawa|   Warszawa|      ochota|           0.0|\n",
            "|  9|Mieszkanie na spr...|    26,90 m²|   0.0|                0.0|      kamienica|          cegła|   1938.0|do zamieszkania|plastikowe|   wtórny|    4.0|  390 000 zł|             26.9|                       0|   390000|             0| 14498.14146953911|          4|   Warszawa|   Warszawa|praga-północ|           0.0|\n",
            "| 11|Mieszkanie na spr...|    80,02 m²|   0.0|                0.0|           blok|           null|   1998.0|           null|      null|   wtórny|   17.0|  900 000 zł|            80.02|                       0|   900000|             0|11247.188674782123|         17|   Warszawa|   Warszawa|        wola|           0.0|\n",
            "| 12|Mieszkanie na spr...|    55,90 m²|   0.0|                0.0|           blok|          cegła|   2001.0|do zamieszkania|plastikowe|   wtórny|    4.0|  569 000 zł|             55.9|                       0|   569000|             0|10178.890598716378|          4|   Warszawa|   Warszawa|    targówek|           0.0|\n",
            "| 13|Mieszkanie na spr...|       47 m²|   0.0|                0.0|           blok|           null|   2003.0|do zamieszkania|plastikowe|   wtórny|    3.0|  499 000 zł|             47.0|                       0|   499000|             0|10617.021276595744|          3|   Warszawa|   Warszawa|     bielany|           0.0|\n",
            "| 14|Mieszkanie na spr...|    30,33 m²|   0.0|                0.0|apartamentowiec|           null|   2009.0|do zamieszkania|      null|   wtórny|   10.0|  479 000 zł|            30.33|                       0|   479000|             0|15792.944319317707|         10|   Warszawa|   Warszawa|     mokotów|           0.0|\n",
            "| 16|Mieszkanie na spr...|    34,95 m²|   0.0|                0.0|      kamienica|          cegła|   1960.0|     do remontu|plastikowe|   wtórny|    4.0|  489 000 zł|            34.95|                       0|   489000|             0|13991.416003587889|          4|   Warszawa|   Warszawa|     mokotów|           0.0|\n",
            "| 18|Mieszkanie na spr...|    71,50 m²|   0.0|                0.0|           blok|           null|      0.0|do zamieszkania|plastikowe|pierwotny|    3.0|  397 000 zł|             71.5|                       0|   397000|             0| 5552.447552447553|          3|legionowski|  wieliszew|            |           0.0|\n",
            "| 19|Mieszkanie na spr...|       74 m²|   0.0|                0.0|apartamentowiec|           null|   2020.0|           null|plastikowe|   wtórny|    8.0|  934 999 zł|             74.0|                       0|   934999|             0|12635.121621621622|          8|   Warszawa|   Warszawa|     mokotów|           0.0|\n",
            "| 21|Mieszkanie na spr...|    35,24 m²|   0.0|                0.0|           null|           null|   2022.0| do wykończenia|plastikowe|pierwotny|    8.0|  383 000 zł|            35.24|                       0|   383000|             0| 10868.33092388954|          8|   Warszawa|   Warszawa|   białołęka|           0.0|\n",
            "| 22|Mieszkanie na spr...|    83,80 m²|   0.0|                0.0|           blok|           inne|      0.0| do wykończenia|plastikowe|pierwotny|    3.0|  779 340 zł|             83.8|                       0|   779340|             0| 9299.999661320446|          3|   Warszawa|   Warszawa|       ursus|           0.0|\n",
            "| 23|Mieszkanie na spr...|       75 m²|   0.0|                0.0|apartamentowiec|         żelbet|   2021.0| do wykończenia|aluminiowe|pierwotny|   23.0|  989 000 zł|             75.0|                       0|   989000|             0|13186.666666666666|         23|   Warszawa|   Warszawa|        wola|           0.0|\n",
            "| 24|Mieszkanie na spr...|    46,70 m²|   0.0|                0.0|           blok|         żelbet|   1976.0|do zamieszkania|      null|   wtórny|    4.0|  565 000 zł|             46.7|                       0|   565000|             0|  12098.5008730102|          4|   Warszawa|   Warszawa|     mokotów|           0.0|\n",
            "+---+--------------------+------------+------+-------------------+---------------+---------------+---------+---------------+----------+---------+-------+------------+-----------------+------------------------+---------+--------------+------------------+-----------+-----------+-----------+------------+--------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "locationDistricts\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-191-577c06f175c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmakeDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-190-781e318fc537>\u001b[0m in \u001b[0;36mmakeDataFrame\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;31m#locationDistricts=locCities(districtsData)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'locationDistricts'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0mconcat_dropped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m     \u001b[0;31m#countyData=concat_dropped.districts.apply(lambda x: funCounties(x))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mconcat_dropped\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mconcat_dropped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"locationCounty\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mudf_funCounties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"districts\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \"\"\"\n\u001b[1;32m    483\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o6464.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 324.0 failed 1 times, most recent failure: Lost task 0.0 in stage 324.0 (TID 324) (ab28497b174f executor driver): net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype)\n\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:773)\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:213)\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:123)\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:136)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$6(BatchEvalPythonExec.scala:94)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n\tat sun.reflect.GeneratedMethodAccessor57.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype)\n\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:773)\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:213)\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:123)\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:136)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$6(BatchEvalPythonExec.scala:94)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArJ_4X1S-67F"
      },
      "source": [
        "type(final)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYrQCg4X-LW3"
      },
      "source": [
        "final.limit(10).toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVY0PJA7Js4T"
      },
      "source": [
        "final.to_csv('/content/drive/My Drive/domymazowieckie/dfMieszkania_final.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwfSbj0CMObA"
      },
      "source": [
        "final.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXBZJwUiQ43J"
      },
      "source": [
        "df=final.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffZlHNQaDKfW"
      },
      "source": [
        "plt.hist(final.loc[:,'lPokoi']/10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jS1BFK5NFkCJ"
      },
      "source": [
        "sns.boxplot(x=(np.log(final[\"powierzchnia_corr\"])/10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6Qi89r8GWTJ"
      },
      "source": [
        "sns.boxplot(x=(np.log(final[\"powierzchniaDzialki_corr\"]+1)/14))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIFvJRm_GlIl"
      },
      "source": [
        "sns.boxplot(x=(np.power(final[\"rokBudowy_corr\"]-1899,4)/3e8))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxoUv1XPKlDA"
      },
      "source": [
        "plt.hist(np.power(final[\"rokBudowy_corr\"]-1899,4)/3e8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NyrjT8CMiny"
      },
      "source": [
        "sns.boxplot(x=final[\"cena/m\"]/20000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHQa0jmgM3tO"
      },
      "source": [
        "sns.boxplot(x=final[\"lPieter_crr\"]/10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpjahNPgNFWG"
      },
      "source": [
        "plt.hist(final[\"lPieter_crr\"]/10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODb0yUQPKDz4"
      },
      "source": [
        "final.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fB9L8WI_NL43"
      },
      "source": [
        "plt.hist((final[\"locationX\"]-21)/4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGiQ-Rv7PUsY"
      },
      "source": [
        "plt.hist((final[\"locationY\"]-52)/2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeO3l4r6DDRX"
      },
      "source": [
        "final.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KEHBeUP0JFb"
      },
      "source": [
        "#min_max_scaler = preprocessing.MinMaxScaler()\n",
        "#data_scaled = min_max_scaler.fit_transform(final.values)\n",
        "#df = pd.DataFrame(data_scaled ,columns=final.columns.values)\n",
        "#df  = df.fillna(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbQNI8K2OjRc"
      },
      "source": [
        "NORMALIZACJA!!!!!!!!!!!!!!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_Rghy5nwEK8"
      },
      "source": [
        "df=final.copy()\n",
        "df['lPokoi']=final.loc[:,'lPokoi']/30\n",
        "df[\"powierzchnia_corr\"]=np.log(final[\"powierzchnia_corr\"])/10\n",
        "df[\"powierzchniaDzialki_corr\"]=np.log(final[\"powierzchniaDzialki_corr\"]+1)/14\n",
        "df[\"rokBudowy_corr\"]=np.power(final[\"rokBudowy_corr\"]-1899,4)/3e8\n",
        "df[\"cena/m\"]=final[\"cena/m\"]/20000\n",
        "df[\"locationX\"]=(final[\"locationX\"]-21)/4\n",
        "df[\"locationY\"]=(final[\"locationY\"]-52)/2\n",
        "df['lPieter_crr']=final.loc[:,'lPieter_crr']/10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18EcJ-8_OqNJ"
      },
      "source": [
        "NORMALIZACJA!!!!!!!!!!!!!!!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYAphnrQSKWb"
      },
      "source": [
        "df  = df.fillna(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yj8AqhOACRNr"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBwn93vEK2hR"
      },
      "source": [
        "#filename_json1='/content/drive/My Drive/domymazowieckie/dfMazowieckieClean.json'\n",
        "#df.to_json(filename_json1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hks9GYwX8Anv"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow as tfCore\n",
        "import math\n",
        "from IPython.core.magic import register_line_magic\n",
        "from IPython.display import Javascript\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YO21H98VWUX"
      },
      "source": [
        "train_dataset = df.sample(frac=0.8, random_state=0)\n",
        "test_dataset = df.drop(df.index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AK8A7ILoVYA3"
      },
      "source": [
        "train_features = train_dataset.copy()\n",
        "test_features = test_dataset.copy()\n",
        "\n",
        "train_labels = train_features.pop('cena/m')\n",
        "test_labels = test_features.pop('cena/m')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNbtSfrMFN41"
      },
      "source": [
        "train_features,test_features,train_labels,test_labels=train_features.values,test_features.values,train_labels.values,test_labels.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXsXSQ5iFzTK"
      },
      "source": [
        "train_features = np.array(train_features, dtype=np.float32)\n",
        "test_features = np.array(test_features, dtype=np.float32)\n",
        "train_labels = np.array(train_labels, dtype=np.float32)\n",
        "test_labels = np.array(test_labels, dtype=np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sm062oKDUEQb"
      },
      "source": [
        "train_features.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tk70_PpYvuPC"
      },
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulbrvU9dwVbD"
      },
      "source": [
        "import timeit\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "  \n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGO50N4lFUe-"
      },
      "source": [
        "model= tf.keras.models.Sequential([\n",
        "                               #    normalizer,\n",
        "  tf.keras.layers.Dense(18*36, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(8*64, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(8*36, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(4*36, activation='relu'),\n",
        "  tf.keras.layers.Dense(1, activation='linear'),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzAtA9nSVfHW"
      },
      "source": [
        "model.compile(loss='mean_absolute_error',\n",
        "              metrics=['mse',\"mae\",'acc'],optimizer=tf.optimizers.Adam(learning_rate=0.01))#learning_rate=0.01"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUvtcdihVg_f"
      },
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  history = model.fit(\n",
        "    train_features, train_labels,\n",
        "    epochs=100,\n",
        "    # suppress logging\n",
        "    verbose=0,\n",
        "    # Calculate validation results on 20% of the training data\n",
        "    validation_split = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLg4iHPBTv7h"
      },
      "source": [
        "plt.plot(history.history['mse'])\n",
        "plt.plot(history.history['val_mse'])\n",
        "plt.ylim(0.01,0.02)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_KszdtnT9lI"
      },
      "source": [
        "plt.plot(history.history['mae'])\n",
        "plt.plot(history.history['val_mae'])\n",
        "plt.ylim(0.080,0.110)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rp13xXZ7kpcw"
      },
      "source": [
        "zabudowa_list = [\"0\", \"dworek/palac\",\"gospodarstwo\",\"kamienica\",\"szeregowiec\",\"wolnostojacy\"]\n",
        "zabudowa = widgets.Dropdown(options=zabudowa_list, value='0')\n",
        "\n",
        "material_list = [\"0\", \"pustak\",\"beton komurkowy\",\"cegla\",\"drewno\",\"inne\",\"karmazyt\",\"pustak\",\"silikat\"]\n",
        "material = widgets.Dropdown(options=material_list, value='0')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkGjMLsDk8ah"
      },
      "source": [
        "zabudowa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uI1hEeymM7-"
      },
      "source": [
        "material"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPyByWWvlA6-"
      },
      "source": [
        "zabudowa_ints=[int(zabudowa.value==element) for element in zabudowa_list]\n",
        "material_ints=[int(material.value==element) for element in material_list]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJIYJghwhO9A"
      },
      "source": [
        "lPokoi = \"8\" #@param {type:\"string\"}\n",
        "powierzchnia = \"100\" #@param {type:\"string\"}\n",
        "powierzchniaDzialki = \"100\" #@param {type:\"string\"}\n",
        "rokBudowy = \"100\" #@param {type:\"string\"}\n",
        "lPieter = \"100\" #@param {type:\"string\"}\n",
        "locationX = \"100\" #@param {type:\"string\"}\n",
        "locationY = \"100\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MF4ggZPyUhiC"
      },
      "source": [
        "lPokoi=4\n",
        "powierzchnia=60\n",
        "powierzchniaDzialki=0\n",
        "rokBudowy=2001\n",
        "lPieter=5\n",
        "locationX=21\n",
        "locationY=52\n",
        "\n",
        "params_norm=[lPokoi/10,np.log(powierzchnia)/10,np.log(powierzchniaDzialki+1)/14,\n",
        "             np.power(rokBudowy-1899,4)/3e8,lPieter/30,\n",
        "             (locationX-21)/4,(locationY-52)/2]\n",
        "\n",
        "parametry=np.array(params_norm)\n",
        "zabudowa=np.array([1,0,0,0,0,0,0])\n",
        "material=np.array([1,0,0,0,0,0,0,0,0])\n",
        "wykonczenie=np.array([1,0,0,0,0,0])\n",
        "okna=np.array([1,0,0,0,0])\n",
        "rynek=np.array([1,0])\n",
        "x=np.concatenate([parametry,zabudowa,material,wykonczenie,okna,rynek])\n",
        "x=np.transpose(np.reshape(x,(-1,1)))\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wn1q4ARxAVfy"
      },
      "source": [
        "cena_m=20000*model.predict(x)[0]\n",
        "cena=cena_m*powierzchnia\n",
        "print(cena_m,' ',cena)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VUoaMzQV-Zd"
      },
      "source": [
        "cena_m=20000*model.predict(x)[0]\n",
        "cena=cena_m*powierzchnia\n",
        "print(cena_m,' ',cena)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WniJVLWTGJ4B"
      },
      "source": [
        "def szybki_podgląd_lokacji(X,Y):\n",
        "  lPokoi=7\n",
        "  powierzchnia=300\n",
        "  powierzchniaDzialki=1000\n",
        "  rokBudowy=2001\n",
        "  lPieter=1\n",
        "  locationX=X\n",
        "  locationY=Y\n",
        "\n",
        "  params_norm=[lPokoi/10,np.log(powierzchnia)/10,np.log(powierzchniaDzialki+1)/14,\n",
        "              np.power(rokBudowy-1899,4)/3e8,lPieter/10,\n",
        "              (locationX-21)/4,(locationY-52)/2]\n",
        "\n",
        "  parametry=np.array(params_norm)\n",
        "  zabudowa=np.array([0,0,0,0,0,0,1])\n",
        "  material=np.array([0,0,1,0,0,0,0,0,0])\n",
        "  wykonczenie=np.array([0,0,0,1,0,0])\n",
        "  okna=np.array([0,0,0,0,1])\n",
        "  rynek=np.array([0,1])\n",
        "  x=np.concatenate([parametry,zabudowa,material,wykonczenie,okna,rynek])\n",
        "  x=np.transpose(np.reshape(x,(-1,1)))\n",
        "  cena_m=20000*model.predict(x)[0]\n",
        "  cena=cena_m*powierzchnia\n",
        "  return cena_m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p56JdCEOGi8P"
      },
      "source": [
        "price=[]\n",
        "for i in range(0,25):\n",
        "  for j in range(0,55):\n",
        "   price.append(szybki_podgląd_lokacji(i,j))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxIG-jRRJ6PJ"
      },
      "source": [
        "price_1=np.array(price).reshape(i+1,j+1)\n",
        "price_2=np.transpose(price_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aya9hV6Lau0S"
      },
      "source": [
        "plt.imshow(price_1, cmap='hot')\n",
        "plt.xlim(40,55)\n",
        "plt.ylim(18,22)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czbHPKkuN3GT"
      },
      "source": [
        "filename='/content/drive/My Drive/domymazowieckie/dfMazowieckie.h5'\n",
        "model.save(filename,save_format='tf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sg6thecPRWzm"
      },
      "source": [
        "pip install tensorflowjs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7s1XngLtQPUy"
      },
      "source": [
        "!tensorflowjs_converter --input_format=keras /content/drive/MyDrive/domymazowieckie/dfMazowieckie.h5 /content/drive/MyDrive/domymazowieckie/jsmodel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xTZrBtUDQ12"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}